{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vagarh/MSC_Eafit/blob/main/lab9-airlines-pyspark_Solucionado.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "XAHgnRUWZSHS",
        "outputId": "2ea30690-910b-4c2c-9ef1-46142248da00",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "#configuración en google colab de spark y pyspark\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "eZgvIu-aZSHT"
      },
      "outputs": [],
      "source": [
        "#instalar java y spark\n",
        "!apt-get install openjdk-11-jdk-headless -qq > /dev/null\n",
        "!wget -q https://downloads.apache.org/spark/spark-3.4.1/spark-3.4.1-bin-hadoop3.tgz\n",
        "!tar xf spark-3.4.1-bin-hadoop3.tgz\n",
        "!pip install -q findspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "X2yguf32ZSHT"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.4.1-bin-hadoop3\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "9_mubuNyZSHT"
      },
      "outputs": [],
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "sc = spark.sparkContext"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "pWCqgQylZSHT"
      },
      "outputs": [],
      "source": [
        "# configuración local de pyspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "gkVmFzkzZSHT"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.functions import monotonically_increasing_id, col, expr, when, concat, lit, isnan\n",
        "from pyspark.ml.linalg import Vectors\n",
        "from pyspark.ml.regression import GeneralizedLinearRegression\n",
        "from pyspark.ml.classification import RandomForestClassifier, LogisticRegression\n",
        "from pyspark.ml.feature import VectorIndexer, VectorAssembler, StringIndexer, OneHotEncoder\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, RegressionEvaluator, BinaryClassificationEvaluator\n",
        "from pyspark.ml import Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "AX8SX_38ZSHU",
        "outputId": "1dd97b27-f9e7-4996-ad1e-8c0a16324cac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 239
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-e77882503e11>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"local\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"airlines\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/spark-3.4.1-bin-hadoop3/python/pyspark/context.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[1;32m    196\u001b[0m             )\n\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m         \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m             self._do_init(\n",
            "\u001b[0;32m/content/spark-3.4.1-bin-hadoop3/python/pyspark/context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m                     \u001b[0;31m# Raise error if there is already a running Spark context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 445\u001b[0;31m                     raise ValueError(\n\u001b[0m\u001b[1;32m    446\u001b[0m                         \u001b[0;34m\"Cannot run multiple SparkContexts at once; \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m                         \u001b[0;34m\"existing SparkContext(app=%s, master=%s)\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=pyspark-shell, master=local[*]) created by getOrCreate at <ipython-input-4-9c757b9b0e87>:4 "
          ]
        }
      ],
      "source": [
        "#from pyspark import SparkContext\n",
        "#sc = SparkContext(\"local\", \"airlines\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "SDp01B2jZSHU"
      },
      "outputs": [],
      "source": [
        "#from pyspark.sql import SparkSession\n",
        "#spark = SparkSession.builder.appName('airlines').getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "7yWw3CBtZSHU",
        "outputId": "6d618556-731a-4d70-ea7e-55fa06cee510",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+---------------+---------+--------+------+--------+-----+-----------+--------------------+\n",
            "|   id|        airline|     date|location|rating|   cabin|value|recommended|              review|\n",
            "+-----+---------------+---------+--------+------+--------+-----+-----------+--------------------+\n",
            "|10001|Delta Air Lines|21-Jun-14|Thailand|     7| Economy|    4|        YES|Flew Mar 30 NRT t...|\n",
            "|10002|Delta Air Lines|19-Jun-14|     USA|     0| Economy|    2|         NO|Flight 2463 leavi...|\n",
            "|10003|Delta Air Lines|18-Jun-14|     USA|     0| Economy|    1|         NO|Delta Website fro...|\n",
            "|10004|Delta Air Lines|17-Jun-14|     USA|     9|Business|    4|        YES|\"I just returned ...|\n",
            "|10005|Delta Air Lines|17-Jun-14| Ecuador|     7| Economy|    3|        YES|\"Round-trip fligh...|\n",
            "|10006|Delta Air Lines|17-Jun-14|     USA|     9|Business|    5|        YES|Narita - Bangkok ...|\n",
            "|10007|Delta Air Lines|14-Jun-14|      UK|     0| Economy|    1|         NO|Flight from NY La...|\n",
            "|10008|Delta Air Lines|14-Jun-14|     USA|     0| Economy|    1|         NO|Originally I had ...|\n",
            "|10009|Delta Air Lines|13-Jun-14|     USA|     4|Business|    2|         NO|We flew paid busi...|\n",
            "|10010|Delta Air Lines|13-Jun-14|      UK|     9| Economy|    3|        YES|\"I flew from Heat...|\n",
            "+-----+---------------+---------+--------+------+--------+-----+-----------+--------------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df=spark.read.csv(\"gdrive/MyDrive/st1800-232/datasets/airlines.csv\", inferSchema=True, header=True)\n",
        "#df=spark.read.csv(\"../datasets/airlines.csv\", inferSchema=True, header=True)\n",
        "\n",
        "df.show(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "kywTlflIZSHU",
        "outputId": "94f9ac79-c842-459f-bd47-097e47eaf5e5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+---------------+---------+---------+------+--------+-----+-----------+--------------------+\n",
            "|   id|        airline|     date| location|rating|   cabin|value|recommended|              review|\n",
            "+-----+---------------+---------+---------+------+--------+-----+-----------+--------------------+\n",
            "|10001|Delta Air Lines|21-Jun-14| Thailand|     7| Economy|    4|        YES|Flew Mar 30 NRT t...|\n",
            "|10002|Delta Air Lines|19-Jun-14|      USA|     0| Economy|    2|         NO|Flight 2463 leavi...|\n",
            "|10003|Delta Air Lines|18-Jun-14|      USA|     0| Economy|    1|         NO|Delta Website fro...|\n",
            "|10004|Delta Air Lines|17-Jun-14|      USA|     9|Business|    4|        YES|\"I just returned ...|\n",
            "|10005|Delta Air Lines|17-Jun-14|  Ecuador|     7| Economy|    3|        YES|\"Round-trip fligh...|\n",
            "|10006|Delta Air Lines|17-Jun-14|      USA|     9|Business|    5|        YES|Narita - Bangkok ...|\n",
            "|10007|Delta Air Lines|14-Jun-14|       UK|     0| Economy|    1|         NO|Flight from NY La...|\n",
            "|10008|Delta Air Lines|14-Jun-14|      USA|     0| Economy|    1|         NO|Originally I had ...|\n",
            "|10009|Delta Air Lines|13-Jun-14|      USA|     4|Business|    2|         NO|We flew paid busi...|\n",
            "|10010|Delta Air Lines|13-Jun-14|       UK|     9| Economy|    3|        YES|\"I flew from Heat...|\n",
            "|10011|Delta Air Lines|11-Jun-14|      USA|    10| Economy|    4|        YES|I was a bit stubb...|\n",
            "|10012|Delta Air Lines|10-Jun-14|Australia|    10| Economy|    5|        YES|JFK-LHR. Had a gr...|\n",
            "|10013|Delta Air Lines| 9-Jun-14|      USA|     0| Economy|    1|         NO|My wife and I fly...|\n",
            "|10014|Delta Air Lines| 9-Jun-14|      USA|    10| Premium|    5|        YES|DL 1134 PBI-ATL. ...|\n",
            "|10015|Delta Air Lines| 6-Jun-14|      USA|     0| Economy|    2|         NO|Our flight from F...|\n",
            "|10016|Delta Air Lines| 5-Jun-14|      USA|     0| Economy|    1|         NO|On May 22 after a...|\n",
            "|10017|Delta Air Lines| 3-Jun-14|   Canada|     9| Economy|    4|        YES|Considering how D...|\n",
            "|10018|Delta Air Lines| 2-Jun-14|      USA|     9| Economy|    5|        YES|Travelled MSP-LHR...|\n",
            "|10019|Delta Air Lines|29-May-14|      USA|     7|Business|    2|        YES|JFK-LAX on a 757-...|\n",
            "|10020|Delta Air Lines|28-May-14|       UK|     9| Economy|    3|        YES|Third long haul f...|\n",
            "+-----+---------------+---------+---------+------+--------+-----+-----------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.createOrReplaceTempView(\"train_df\")\n",
        "sqlDF = spark.sql(\"SELECT * FROM train_df\")\n",
        "sqlDF.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "XyPsuVCGZSHU"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import codecs\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "3Fm8TTTQZSHU",
        "outputId": "d7220ec5-96f3-48f7-94a8-e5c05bb77985",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "8kz1sA1kZSHU"
      },
      "outputs": [],
      "source": [
        "\n",
        "# stopwords en nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "stop_words_nltk = set(stopwords.words('english'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "syBScXCPZSHU"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.feature import HashingTF, IDF, Tokenizer, CountVectorizer\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.ml.linalg import Vectors, SparseVector\n",
        "from pyspark.ml.clustering import LDA, BisectingKMeans\n",
        "from pyspark.sql.functions import monotonically_increasing_id\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "LWaSubi2ZSHU",
        "outputId": "0448afb8-186c-49ad-fe56-4043431cf20d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- id: string (nullable = true)\n",
            " |-- airline: string (nullable = true)\n",
            " |-- date: string (nullable = true)\n",
            " |-- location: string (nullable = true)\n",
            " |-- rating: string (nullable = true)\n",
            " |-- cabin: string (nullable = true)\n",
            " |-- value: string (nullable = true)\n",
            " |-- recommended: string (nullable = true)\n",
            " |-- review: string (nullable = true)\n",
            "\n",
            "+-----+---------------+---------+--------+------+--------+-----+-----------+--------------------+---+----------+\n",
            "|   id|        airline|     date|location|rating|   cabin|value|recommended|              review|uid|year_month|\n",
            "+-----+---------------+---------+--------+------+--------+-----+-----------+--------------------+---+----------+\n",
            "|10001|Delta Air Lines|21-Jun-14|Thailand|     7| Economy|    4|        YES|Flew Mar 30 NRT t...|  0|    21-Jun|\n",
            "|10002|Delta Air Lines|19-Jun-14|     USA|     0| Economy|    2|         NO|Flight 2463 leavi...|  1|    19-Jun|\n",
            "|10003|Delta Air Lines|18-Jun-14|     USA|     0| Economy|    1|         NO|Delta Website fro...|  2|    18-Jun|\n",
            "|10004|Delta Air Lines|17-Jun-14|     USA|     9|Business|    4|        YES|\"I just returned ...|  3|    17-Jun|\n",
            "|10005|Delta Air Lines|17-Jun-14| Ecuador|     7| Economy|    3|        YES|\"Round-trip fligh...|  4|    17-Jun|\n",
            "|10006|Delta Air Lines|17-Jun-14|     USA|     9|Business|    5|        YES|Narita - Bangkok ...|  5|    17-Jun|\n",
            "|10007|Delta Air Lines|14-Jun-14|      UK|     0| Economy|    1|         NO|Flight from NY La...|  6|    14-Jun|\n",
            "|10008|Delta Air Lines|14-Jun-14|     USA|     0| Economy|    1|         NO|Originally I had ...|  7|    14-Jun|\n",
            "|10009|Delta Air Lines|13-Jun-14|     USA|     4|Business|    2|         NO|We flew paid busi...|  8|    13-Jun|\n",
            "|10010|Delta Air Lines|13-Jun-14|      UK|     9| Economy|    3|        YES|\"I flew from Heat...|  9|    13-Jun|\n",
            "+-----+---------------+---------+--------+------+--------+-----+-----------+--------------------+---+----------+\n",
            "only showing top 10 rows\n",
            "\n",
            "('id', 'string')\n",
            "('airline', 'string')\n",
            "('date', 'string')\n",
            "('location', 'string')\n",
            "('rating', 'string')\n",
            "('cabin', 'string')\n",
            "('value', 'string')\n",
            "('recommended', 'string')\n",
            "('review', 'string')\n",
            "('uid', 'bigint')\n",
            "('year_month', 'string')\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('rating', 'int')]"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "rawdata = spark.read.load(\"/content/gdrive/MyDrive/st1800-232/datasets/airlines.csv\", format=\"csv\", header=True)\n",
        "rawdata.printSchema()\n",
        "rawdata[0]\n",
        "rawdata = rawdata.fillna({'review': ''})                               # Replace nulls with blank string\n",
        "\n",
        "# Add Unique ID\n",
        "rawdata = rawdata.withColumn(\"uid\", monotonically_increasing_id())     # Create Unique ID\n",
        "\n",
        "# Generate YYYY-MM variable\n",
        "rawdata = rawdata.withColumn(\"year_month\", rawdata.date.substr(1,6))\n",
        "\n",
        "# Show rawdata (as DataFrame)\n",
        "rawdata.show(10)\n",
        "\n",
        "# Print data types\n",
        "for type in rawdata.dtypes:\n",
        "    print(type)\n",
        "\n",
        "target = rawdata.select(rawdata['rating'].cast(IntegerType()))\n",
        "target.dtypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "VU23Pn3tZSHU",
        "outputId": "aaa54ee4-9568-47d0-a8cf-e55e39ed92b7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+---------------+---------+--------+------+--------+-----+-----------+--------------------+-------------------+\n",
            "|   id|        airline|     date|location|rating|   cabin|value|recommended|              review|              words|\n",
            "+-----+---------------+---------+--------+------+--------+-----+-----------+--------------------+-------------------+\n",
            "|10001|Delta Air Lines|21-Jun-14|Thailand|     7| Economy|    4|        YES|Flew Mar 30 NRT t...|[delta, air, lines]|\n",
            "|10002|Delta Air Lines|19-Jun-14|     USA|     0| Economy|    2|         NO|Flight 2463 leavi...|[delta, air, lines]|\n",
            "|10003|Delta Air Lines|18-Jun-14|     USA|     0| Economy|    1|         NO|Delta Website fro...|[delta, air, lines]|\n",
            "|10004|Delta Air Lines|17-Jun-14|     USA|     9|Business|    4|        YES|\"I just returned ...|[delta, air, lines]|\n",
            "|10005|Delta Air Lines|17-Jun-14| Ecuador|     7| Economy|    3|        YES|\"Round-trip fligh...|[delta, air, lines]|\n",
            "+-----+---------------+---------+--------+------+--------+-----+-----------+--------------------+-------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "################################################################################################\n",
        "#\n",
        "#   Text Pre-processing (consider using one or all of the following):\n",
        "#       - Remove common words (with stoplist)\n",
        "#       - Handle punctuation\n",
        "#       - lowcase/upcase\n",
        "#       - Stemming\n",
        "#       - Part-of-Speech Tagging (nouns, verbs, adj, etc.)\n",
        "#\n",
        "################################################################################################\n",
        "from pyspark.sql.functions import udf,struct\n",
        "#from pyspark.sql.types import StructType\n",
        "def textprep(record):\n",
        "    text  = record[1]\n",
        "    uid   = record[0]\n",
        "    tokens = text.split()\n",
        "\n",
        "    # Custom List of Stopwords - Add your own here\n",
        "    tokens = [re.sub('[^a-zA-Z0-9]','',word) for word in tokens]                                       # Remove special characters\n",
        "    tokens = [word.lower() for word in tokens if len(word)>2 and word.lower() not in stop_words_nltk]     # Remove stopwords and words under X length\n",
        "    return tokens\n",
        "\n",
        "udf_textprep = udf(textprep , ArrayType(StringType()))\n",
        "df = df.withColumn(\"words\", udf_textprep(struct([df[x] for x in df.columns])))\n",
        "\n",
        "#tokenizer = Tokenizer(inputCol=\"description\", outputCol=\"words\")\n",
        "#wordsData = tokenizer.transform(text)\n",
        "df.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "jr4uX33vZSHV",
        "outputId": "01a0e6a6-e4c2-4292-e481-622b2cbb85d6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+---------------+---------+--------+------+--------+-----+-----------+--------------------+-------------------+--------------------+--------------------+\n",
            "|   id|        airline|     date|location|rating|   cabin|value|recommended|              review|              words|         rawFeatures|            features|\n",
            "+-----+---------------+---------+--------+------+--------+-----+-----------+--------------------+-------------------+--------------------+--------------------+\n",
            "|10001|Delta Air Lines|21-Jun-14|Thailand|     7| Economy|    4|        YES|Flew Mar 30 NRT t...|[delta, air, lines]|(8,[1,2,3],[1.0,1...|(8,[1,2,3],[1.387...|\n",
            "|10002|Delta Air Lines|19-Jun-14|     USA|     0| Economy|    2|         NO|Flight 2463 leavi...|[delta, air, lines]|(8,[1,2,3],[1.0,1...|(8,[1,2,3],[1.387...|\n",
            "|10003|Delta Air Lines|18-Jun-14|     USA|     0| Economy|    1|         NO|Delta Website fro...|[delta, air, lines]|(8,[1,2,3],[1.0,1...|(8,[1,2,3],[1.387...|\n",
            "|10004|Delta Air Lines|17-Jun-14|     USA|     9|Business|    4|        YES|\"I just returned ...|[delta, air, lines]|(8,[1,2,3],[1.0,1...|(8,[1,2,3],[1.387...|\n",
            "|10005|Delta Air Lines|17-Jun-14| Ecuador|     7| Economy|    3|        YES|\"Round-trip fligh...|[delta, air, lines]|(8,[1,2,3],[1.0,1...|(8,[1,2,3],[1.387...|\n",
            "|10006|Delta Air Lines|17-Jun-14|     USA|     9|Business|    5|        YES|Narita - Bangkok ...|[delta, air, lines]|(8,[1,2,3],[1.0,1...|(8,[1,2,3],[1.387...|\n",
            "|10007|Delta Air Lines|14-Jun-14|      UK|     0| Economy|    1|         NO|Flight from NY La...|[delta, air, lines]|(8,[1,2,3],[1.0,1...|(8,[1,2,3],[1.387...|\n",
            "|10008|Delta Air Lines|14-Jun-14|     USA|     0| Economy|    1|         NO|Originally I had ...|[delta, air, lines]|(8,[1,2,3],[1.0,1...|(8,[1,2,3],[1.387...|\n",
            "|10009|Delta Air Lines|13-Jun-14|     USA|     4|Business|    2|         NO|We flew paid busi...|[delta, air, lines]|(8,[1,2,3],[1.0,1...|(8,[1,2,3],[1.387...|\n",
            "|10010|Delta Air Lines|13-Jun-14|      UK|     9| Economy|    3|        YES|\"I flew from Heat...|[delta, air, lines]|(8,[1,2,3],[1.0,1...|(8,[1,2,3],[1.387...|\n",
            "+-----+---------------+---------+--------+------+--------+-----+-----------+--------------------+-------------------+--------------------+--------------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Term Frequency Vectorization  - Option 1 (Using hashingTF):\n",
        "#hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=20)\n",
        "#featurizedData = hashingTF.transform(clean_text)\n",
        "\n",
        "# Term Frequency Vectorization  - Option 2 (CountVectorizer)    :\n",
        "cv = CountVectorizer(inputCol=\"words\", outputCol=\"rawFeatures\", vocabSize = 1000)\n",
        "cvmodel = cv.fit(df)\n",
        "featurizedData = cvmodel.transform(df)\n",
        "\n",
        "vocab = cvmodel.vocabulary\n",
        "vocab_broadcast = sc.broadcast(vocab)\n",
        "\n",
        "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
        "idfModel = idf.fit(featurizedData)\n",
        "rescaledData = idfModel.transform(featurizedData)\n",
        "rescaledData.show(10)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.10.6 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
      }
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}